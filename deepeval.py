from deepeval.metrics import SummarizationMetric, BiasMetric, ToxicityMetric
from deepeval.test_case import LLMTestCase
from deepeval import evaluate
class evaluator_agent:
    def __init__(self):
        self.llm = AzureChatOpenAI()
        self.model = AzureOpenAI(model=self.llm)
        
    def evaluate(self, summary: str, reference: str) -> dict:
        metrics = [
            SummarizationMetric(model=self.model),
            BiasMetric(model=self.model),
            ToxicityMetric(model=self.model)
        ]
        test_case = LLMTestCase(input=reference, actual_output=summary)

        results = evaluate(test_cases=[test_case], metrics=metrics)
        ans = {}
        for i in results.test_results[0].metrics_data:
            ans[i.name] = {
                "score": i.score,
                "reason": i.reason
            }
        return ans
    
evaluator_agent().evaluate("This is a placeholder summary generated by the model.", "This is a placeholder reference summary.")

